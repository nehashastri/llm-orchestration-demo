# LLM Orchestration API - Environment Variables
# Copy this file to .env and fill in the actual values

# ============================================================================
# REQUIRED: LLM API Keys
# ============================================================================

# OpenAI API Key - Required for GPT models
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your-key-here

# Anthropic API Key - Required for Claude models (optional)
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-key-here

# ============================================================================
# Application Settings
# ============================================================================

# Environment mode: development, staging, production
ENVIRONMENT=production

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# Redis Configuration
# ============================================================================

# Redis host (use 'redis' for docker-compose, 'localhost' for local dev)
REDIS_HOST=redis

# Redis port
REDIS_PORT=6379

# Redis database number (0-15)
REDIS_DB=0

# ============================================================================
# LLM Model Defaults
# ============================================================================

# Default model to use when not specified in requests
DEFAULT_MODEL=gpt-4o

# Default temperature for model responses (0.0 - 2.0)
DEFAULT_TEMPERATURE=0.7

# Default maximum tokens for model responses
DEFAULT_MAX_TOKENS=500

# Default timeout for API calls (seconds)
DEFAULT_TIMEOUT=30

# ============================================================================
# Rate Limiting
# ============================================================================

# Maximum number of requests per window
RATE_LIMIT_REQUESTS=100

# Time window for rate limiting (seconds)
RATE_LIMIT_WINDOW=3600
