Production-ready FastAPI service for orchestrating OpenAI models with parallel execution and fallback strategies.

‚ú® Features

üîÑ OpenAI Integration: GPT-4o/4o-mini/3.5-turbo models
‚ö° Async-First Architecture: Non-blocking I/O for maximum throughput
üéØ Intelligent Fallback: Automatic fallback within the OpenAI model family with default message safety net
üìä Built-in Observability: Structured logging, cost tracking, latency monitoring
üõ°Ô∏è Production-Ready: Error handling, rate limiting, request validation
üìö Auto-Generated Docs: Interactive Swagger UI at /docs


üé¨ Quick Start
Prerequisites

Python 3.11+
Pixi
OpenAI API key

1. Clone the Repository
```bash
git clone https://github.com/yourusername/llm-orchestration-demo.git
cd llm-orchestration-demo
```

2. Install Dependencies
```bash
pixi install
```

3. Configure Environment
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
```

4. Run the Server
```bash
pixi run dev
```
Server starts at: http://localhost:8000

5. View Interactive Docs
```bash
start http://localhost:8000/docs
```

Project Overview

**Key Directories:**
- `src/api/` ‚Äî FastAPI application (routes, models, middleware)
- `src/llm/` ‚Äî LLM orchestration (clients, orchestrator strategies)
- `src/utils/` ‚Äî Configuration, logging, utilities
- `tests/` ‚Äî Pytest test suite
- `examples/` ‚Äî Working code examples (basic, parallel, streaming, fallback)
- `docs/` ‚Äî Architecture and API specifications

üéØ Usage Examples
Basic Chat
pythonimport requests

response = requests.post(
    "http://localhost:8000/chat",
    json={
        "prompt": "Explain async programming in Python",
        "model": "gpt-4-turbo",
        "temperature": 0.7,
        "max_tokens": 500
    }
)

print(response.json()["content"])
## Parallel Orchestration
Runs multiple OpenAI models in parallel (e.g., gpt-4o vs gpt-4o-mini) and returns the fastest success.
Fallback Strategy
python# Automatic fallback: gpt-4-turbo ‚Üí gpt-3.5-turbo ‚Üí default message
response = requests.post(
    "http://localhost:8000/chat/fallback",
    json={
        "prompt": "What is quantum computing?",
        "primary_provider": "openai",
        "primary_model": "gpt-4-turbo",
        "timeout": 10
    }
)

result = response.json()
print(f"Provider used: {result['provider_used']}")
print(f"Fallback triggered: {result['fallback_triggered']}")
print(f"Is default message: {result['is_default_message']}")
Streaming Response
pythonimport sseclient
import requests

response = requests.post(
    "http://localhost:8000/chat/stream",
    json={"prompt": "Tell me a story"},
    stream=True
)

client = sseclient.SSEClient(response)
for event in client.events():
    if event.data == '[DONE]':
        break
    print(event.data, end='', flush=True)

üß™ Testing
```bash
pixi run test                    # Run full suite
pixi run test-cov                # With HTML coverage report
pixi run -e dev pytest tests/test_api.py  # Single test file
```

üîß Development
```bash
pixi run format                  # Format code + markdown
pixi run lint                    # Lint and auto-fix
pixi run typecheck               # Type checking
pixi run check                   # All checks: format, lint, typecheck
```

For comprehensive development guidance, see [COPILOT.md](COPILOT.md) ‚Äî covers patterns, testing strategy, error handling, and documentation standards.

üìä Monitoring & Observability
Structured Logging
All requests are logged in JSON format:
json{
  "timestamp": "2024-01-15T10:30:45Z",
  "level": "info",
  "event": "llm_call",
  "request_id": "req_abc123",
  "model": "gpt-4-turbo",
  "latency_ms": 1234,
  "cost_usd": 0.0045,
  "prompt_tokens": 150,
  "completion_tokens": 300
}
Log Files
The service writes logs to the `logs` directory with daily rotation:
- Active file: `logs/app.log`
- Rotated files: `logs/app-YYYY-MM-DD.log` (e.g., `app-2025-12-21.log`)
At midnight, the current `app.log` rolls over to the dated file, so yesterday's logs are always available as a separate file.
View Statistics
bashcurl http://localhost:8000/stats
Returns:

Total requests
Average latency
Total cost (OpenAI only)
Error rate


üöÄ Deployment

### Local Development
```bash
pixi install
cp .env.example .env
# Edit .env and add your OpenAI API key
pixi run dev
```

### Docker Deployment
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
docker-compose up -d
```

**Verify deployment:**
```bash
curl http://localhost:8000/health
curl http://localhost:8000/models
```

**Access the API:**
- API: http://localhost:8000
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

#### Environment Variables
See `.env.example` for all available configuration options. Key variables:
- `OPENAI_API_KEY` - **Required** for OpenAI models
- `ANTHROPIC_API_KEY` - Optional for Anthropic/Claude models
- `ENVIRONMENT` - production/development/testing (default: production)
- `LOG_LEVEL` - DEBUG/INFO/WARNING/ERROR (default: INFO)
- `REDIS_HOST` - Redis hostname (default: redis for docker-compose, localhost for local dev)
- `DEFAULT_MODEL` - Default LLM model (default: gpt-4o)
- `RATE_LIMIT_REQUESTS` - Requests per window (default: 100)

For comprehensive Docker deployment guidance, see [DOCKER.md](DOCKER.md)


üìö Documentation

- **Development Guide:** [COPILOT.md](COPILOT.md) ‚Äî Patterns, code style, testing, how-to guides
- **Architecture:** [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) ‚Äî System design and data flows
- **API Specs:** [docs/api_specs.md](docs/api_specs.md) ‚Äî Endpoint reference
- **Interactive Docs:** http://localhost:8000/docs (when running)


ü§ù Contributing

Fork the repository
Create a feature branch (git checkout -b feature/amazing-feature)
Make your changes
Run tests (pixi run test)
Format code (pixi run format)
Commit changes (git commit -m 'Add amazing feature')
Push to branch (git push origin feature/amazing-feature)
Open a Pull Request

Development Setup
bash# Install pre-commit hooks
pixi run pre-commit install

# Run all checks before committing
pixi run pre-commit run --all-files

üìù License
This project is licensed under the MIT License - see LICENSE file for details.

üôè Acknowledgments

FastAPI - Modern web framework
OpenAI - GPT models
